{
  "questions": [
    {
      "id": 1,
      "difficulty": "Medium",
      "question": "Which assumption is made by the Naive Bayes classifier?",
      "options": [
        "Features are linearly separable",
        "Features are independent given the class",
        "Classes have equal prior probability",
        "Data follows uniform distribution"
      ],
      "answer": "Features are independent given the class"
    },
    {
      "id": 2,
      "difficulty": "Medium",
      "question": "In KNN, increasing the value of K generally results in:",
      "options": [
        "Higher variance and lower bias",
        "Lower variance and higher bias",
        "Higher variance and higher bias",
        "Lower variance and lower bias"
      ],
      "answer": "Lower variance and higher bias"
    },
    {
      "id": 3,
      "difficulty": "Hard",
      "question": "Which kernel trick allows SVM to handle non-linearly separable data?",
      "options": [
        "Gradient Descent",
        "Lagrange Optimization",
        "Feature Space Transformation",
        "Cost Function Regularization"
      ],
      "answer": "Feature Space Transformation"
    },
    {
      "id": 4,
      "difficulty": "Medium",
      "question": "Which metric is best suited for evaluating an imbalanced classification problem?",
      "options": [
        "Accuracy",
        "Mean Squared Error",
        "F1-Score",
        "R-squared"
      ],
      "answer": "F1-Score"
    },
    {
      "id": 5,
      "difficulty": "Hard",
      "question": "Which regularization technique adds the absolute value of coefficients to the loss function?",
      "options": [
        "Ridge Regression",
        "Lasso Regression",
        "Elastic Net",
        "Polynomial Regression"
      ],
      "answer": "Lasso Regression"
    },
    {
      "id": 6,
      "difficulty": "Medium",
      "question": "Random Forest reduces overfitting mainly by:",
      "options": [
        "Using deeper trees",
        "Averaging multiple decision trees",
        "Reducing feature size",
        "Using gradient descent"
      ],
      "answer": "Averaging multiple decision trees"
    },
    {
      "id": 7,
      "difficulty": "Hard",
      "question": "In PCA, the principal components are chosen such that they:",
      "options": [
        "Maximize the mean of data",
        "Minimize classification error",
        "Maximize variance",
        "Minimize covariance"
      ],
      "answer": "Maximize variance"
    },
    {
      "id": 8,
      "difficulty": "Medium",
      "question": "Which stopping criterion is commonly used in Decision Trees?",
      "options": [
        "Learning rate becomes zero",
        "Maximum tree depth is reached",
        "All features are used",
        "Distance metric converges"
      ],
      "answer": "Maximum tree depth is reached"
    },
    {
      "id": 9,
      "difficulty": "Hard",
      "question": "Which concept explains why deep neural networks perform better with large datasets?",
      "options": [
        "Bias-Variance Tradeoff",
        "Curse of Dimensionality",
        "Universal Approximation Theorem",
        "No Free Lunch Theorem"
      ],
      "answer": "Universal Approximation Theorem"
    },
    {
      "id": 10,
      "difficulty": "Medium",
      "question": "Which technique is commonly used to handle missing values in datasets?",
      "options": [
        "Feature scaling",
        "Data augmentation",
        "Imputation",
        "Dimensionality reduction"
      ],
      "answer": "Imputation"
    }
  ]
}
